{"abstracts-retrieval-response": {
    "item": {
        "ait:process-info": {
            "ait:status": {
                "@state": "update",
                "@type": "core",
                "@stage": "S300"
            },
            "ait:date-delivered": {
                "@day": "11",
                "@timestamp": "2022-07-11T14:36:54.000054-04:00",
                "@year": "2022",
                "@month": "07"
            },
            "ait:date-sort": {
                "@day": "01",
                "@year": "2020",
                "@month": "06"
            }
        },
        "xocs:meta": {"xocs:funding-list": {
            "@pui-match": "primary",
            "@has-funding-info": "1",
            "xocs:funding-addon-generated-timestamp": "2021-06-05T01:15:37.722Z",
            "xocs:funding-addon-type": "http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/aggregated-refined"
        }},
        "bibrecord": {
            "head": {
                "author-group": {
                    "affiliation": {
                        "country": "Thailand",
                        "@afid": "60199583",
                        "@country": "tha",
                        "city": "Bangkok",
                        "organization": [
                            {"$": "Chulalongkorn University"},
                            {"$": "Chulalongkorn Business School"}
                        ],
                        "affiliation-id": [
                            {"@afid": "60199583"},
                            {"@afid": "60028190"}
                        ],
                        "ce:source-text": "Chulalongkorn University,Chulalongkorn Business School,Bangkok,Thailand"
                    },
                    "author": [
                        {
                            "ce:given-name": "Janjao",
                            "preferred-name": {
                                "ce:given-name": "Janjao",
                                "ce:initials": "J.",
                                "ce:surname": "Mongkolnavin",
                                "ce:indexed-name": "Mongkolnavin J."
                            },
                            "@seq": "1",
                            "ce:initials": "J.",
                            "@date-locked": "2021-01-30T02:57:14.829",
                            "@_fa": "true",
                            "@type": "auth",
                            "ce:surname": "Mongkolnavin",
                            "@auid": "25922814600",
                            "ce:indexed-name": "Mongkolnavin J."
                        },
                        {
                            "ce:given-name": "Widakorn",
                            "preferred-name": {
                                "ce:given-name": "Widakorn",
                                "ce:initials": "W.",
                                "ce:surname": "Saewong",
                                "ce:indexed-name": "Saewong W."
                            },
                            "@seq": "2",
                            "ce:initials": "W.",
                            "@_fa": "true",
                            "@type": "auth",
                            "ce:surname": "Saewong",
                            "@auid": "57211507692",
                            "ce:indexed-name": "Saewong W."
                        }
                    ]
                },
                "citation-title": "Prediction of Forthcoming Anger of Customer in Call Center Dialogs",
                "abstracts": "© 2020 IEEE.Call center is a department that is most relevant to audio data usage. One of its major tasks is to monitor customers' anguish because it has a negative impact on the organization. One challenging task is to develop a model that can predict whether a customer is getting angry in the next turn of conversation. Such model can assist agents in taking appropriate action(s) to prevent the incidents. In this study, we investigate an approach to build an anger prediction model from customers' voice in call center dialogs. To create the model requires 5 processes: (1) Customer's turn extraction (2) Emotion annotation (3) Voice feature selection (4) Data pre-processing for long short-term memory networks, and (5) Anger prediction modeling. Five long short-term memory networks were built with the time series data sets of 1, 2, 3, 4, and 5 consecutive turns. The experimental results showed that the long short-term memory network built with the 3-consecutive turn data has promising performance in aspect of Average Precision and False Negative Rate when compared to the random and good guess benchmarks.",
                "citation-info": {
                    "author-keywords": {"author-keyword": [
                        {
                            "$": "Anger Prediction",
                            "@xml:lang": "eng",
                            "@original": "y"
                        },
                        {
                            "$": "Call center dialogs",
                            "@xml:lang": "eng",
                            "@original": "y"
                        },
                        {
                            "$": "Emotion Recognition",
                            "@xml:lang": "eng",
                            "@original": "y"
                        },
                        {
                            "$": "Long Short-term Memory Networks",
                            "@xml:lang": "eng",
                            "@original": "y"
                        },
                        {
                            "$": "Machine Learning",
                            "@xml:lang": "eng",
                            "@original": "y"
                        },
                        {
                            "$": "Recurrent Neural Networks",
                            "@xml:lang": "eng",
                            "@original": "y"
                        }
                    ]},
                    "citation-type": {"@code": "cp"},
                    "citation-language": {
                        "@language": "English",
                        "@xml:lang": "eng"
                    },
                    "abstract-language": {
                        "@language": "English",
                        "@xml:lang": "eng"
                    }
                },
                "source": {
                    "website": {"ce:e-address": {
                        "$": "http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=9146972",
                        "@type": "email"
                    }},
                    "translated-sourcetitle": {
                        "$": "17th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology, ECTI-CON 2020",
                        "@xml:lang": "eng"
                    },
                    "volisspag": {"pagerange": {
                        "@first": "779",
                        "@last": "783"
                    }},
                    "@type": "p",
                    "isbn": {
                        "@level": "volume",
                        "$": "9781728164861",
                        "@type": "electronic",
                        "@length": "13"
                    },
                    "additional-srcinfo": {"conferenceinfo": {
                        "confpublication": {"procpartno": "1 of 1"},
                        "confevent": {
                            "confname": "17th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology, ECTI-CON 2020",
                            "confnumber": "17",
                            "conforganization": "Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI) Association",
                            "confcatnumber": "CFP2006E-ART",
                            "confseriestitle": "International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology",
                            "conflocation": {
                                "@country": "tha",
                                "city": "Virtual, Online"
                            },
                            "confcode": "162286",
                            "confdate": {
                                "enddate": {
                                    "@day": "27",
                                    "@year": "2020",
                                    "@month": "06"
                                },
                                "startdate": {
                                    "@day": "24",
                                    "@year": "2020",
                                    "@month": "06"
                                }
                            }
                        }
                    }},
                    "sourcetitle": "17th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology, ECTI-CON 2020",
                    "publicationdate": {
                        "month": "06",
                        "year": "2020",
                        "date-text": "June 2020",
                        "day": "01"
                    },
                    "sourcetitle-abbrev": "Int. Conf. Electr. Eng./Electron., Comput., Telecommun. Inf. Technol., ECTI-CON",
                    "@country": "usa",
                    "issuetitle": "17th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology, ECTI-CON 2020",
                    "publicationyear": {"@first": "2020"},
                    "publisher": {
                        "affiliation": {
                            "@country": "usa",
                            "city": null
                        },
                        "publishername": "Institute of Electrical and Electronics Engineers Inc."
                    },
                    "article-number": "9158120",
                    "@srcid": "21101022467"
                },
                "enhancement": {"classificationgroup": {"classifications": [
                    {
                        "@type": "ASJC",
                        "classification": [
                            {"$": "1702"},
                            {"$": "1705"},
                            {"$": "1708"},
                            {"$": "1712"},
                            {"$": "1802"},
                            {"$": "2208"},
                            {"$": "3105"}
                        ]
                    },
                    {
                        "@type": "CPXCLASS",
                        "classification": {
                            "classification-code": "461.1",
                            "classification-description": "Biomedical Engineering"
                        }
                    },
                    {
                        "@type": "FLXCLASS",
                        "classification": {
                            "classification-code": "902",
                            "classification-description": "FLUIDEX; Related Topics"
                        }
                    },
                    {
                        "@type": "SUBJABBR",
                        "classification": [
                            {"$": "COMP"},
                            {"$": "DECI"},
                            {"$": "ENGI"},
                            {"$": "PHYS"}
                        ]
                    }
                ]}}
            },
            "item-info": {
                "copyright": {
                    "$": "Copyright 2020 Elsevier B.V., All rights reserved.",
                    "@type": "Elsevier"
                },
                "dbcollection": [
                    {"$": "CPX"},
                    {"$": "SCOPUS"},
                    {"$": "Scopusbase"}
                ],
                "history": {"date-created": {
                    "@day": "06",
                    "@timestamp": "BST 07:06:43",
                    "@year": "2020",
                    "@month": "10"
                }},
                "itemidlist": {
                    "itemid": [
                        {
                            "$": "633011609",
                            "@idtype": "PUI"
                        },
                        {
                            "$": "928761189",
                            "@idtype": "CAR-ID"
                        },
                        {
                            "$": "20204009300647",
                            "@idtype": "CPX"
                        },
                        {
                            "$": "20203501093",
                            "@idtype": "SCOPUS"
                        },
                        {
                            "$": "85091889483",
                            "@idtype": "SCP"
                        },
                        {
                            "$": "85091889483",
                            "@idtype": "SGR"
                        }
                    ],
                    "ce:doi": "10.1109/ECTI-CON49241.2020.9158120"
                }
            },
            "tail": {"bibliography": {
                "@refcount": "26",
                "reference": [
                    {
                        "ref-fulltext": "J. Bhaskar, K. Sruthi, and P. Nedungadi, \"Hybrid approach for emotion classification of audio conversation based on text and speech mining, \" Procedia Computer Science, vol. 46, pp. 635-643, 2015.",
                        "@id": "1",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2015"},
                            "ref-title": {"ref-titletext": "Hybrid approach for emotion classification of audio conversation based on text and speech mining"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84931375757",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "46"},
                                "pagerange": {
                                    "@first": "635",
                                    "@last": "643"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Bhaskar",
                                    "ce:indexed-name": "Bhaskar J."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "K.",
                                    "@_fa": "true",
                                    "ce:surname": "Sruthi",
                                    "ce:indexed-name": "Sruthi K."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "P.",
                                    "@_fa": "true",
                                    "ce:surname": "Nedungadi",
                                    "ce:indexed-name": "Nedungadi P."
                                }
                            ]},
                            "ref-sourcetitle": "Procedia Computer Science"
                        },
                        "ce:source-text": "J. Bhaskar, K. Sruthi, and P. Nedungadi, \"Hybrid approach for emotion classification of audio conversation based on text and speech mining, \" Procedia Computer Science, vol. 46, pp. 635-643, 2015."
                    },
                    {
                        "ref-fulltext": "R. Chakraborty, M. Pandharipande, and S. Kopparapu, \"Mining call center conversations exhibiting similar affective states, \" in Proc. 30th Pacific Asia Conf. Lang., Information and Computation: Posters, Oct. 2016, pp. 545-553.",
                        "@id": "2",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2016"},
                            "ref-title": {"ref-titletext": "Mining call center conversations exhibiting similar affective states"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85015933239",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "545",
                                "@last": "553"
                            }},
                            "ref-text": "Oct.",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "R.",
                                    "@_fa": "true",
                                    "ce:surname": "Chakraborty",
                                    "ce:indexed-name": "Chakraborty R."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Pandharipande",
                                    "ce:indexed-name": "Pandharipande M."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Kopparapu",
                                    "ce:indexed-name": "Kopparapu S."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. 30th Pacific Asia Conf. Lang., Information and Computation: Posters"
                        },
                        "ce:source-text": "R. Chakraborty, M. Pandharipande, and S. Kopparapu, \"Mining call center conversations exhibiting similar affective states, \" in Proc. 30th Pacific Asia Conf. Lang., Information and Computation: Posters, Oct. 2016, pp. 545-553."
                    },
                    {
                        "ref-fulltext": "A. Jaumard-Hakoun, K. Xu, C. Leboullenger, P. Roussel-Ragot, and B. Denby, \"An articulatory-based singing voice synthesis using tongue and lips imaging, \" presented at the ISCA Interspeech 2016, San Francisco, United States, Sep. 2016.",
                        "@id": "3",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2016"},
                            "ref-title": {"ref-titletext": "An articulatory-based singing voice synthesis using tongue and lips imaging"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84994246978",
                                "@idtype": "SGR"
                            }},
                            "ref-text": "San Francisco, United States, Sep.",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Jaumard-Hakoun",
                                    "ce:indexed-name": "Jaumard-Hakoun A."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "K.",
                                    "@_fa": "true",
                                    "ce:surname": "Xu",
                                    "ce:indexed-name": "Xu K."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "C.",
                                    "@_fa": "true",
                                    "ce:surname": "Leboullenger",
                                    "ce:indexed-name": "Leboullenger C."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "P.",
                                    "@_fa": "true",
                                    "ce:surname": "Roussel-Ragot",
                                    "ce:indexed-name": "Roussel-Ragot P."
                                },
                                {
                                    "@seq": "5",
                                    "ce:initials": "B.",
                                    "@_fa": "true",
                                    "ce:surname": "Denby",
                                    "ce:indexed-name": "Denby B."
                                }
                            ]},
                            "ref-sourcetitle": "ISCA Interspeech 2016"
                        },
                        "ce:source-text": "A. Jaumard-Hakoun, K. Xu, C. Leboullenger, P. Roussel-Ragot, and B. Denby, \"An articulatory-based singing voice synthesis using tongue and lips imaging, \" presented at the ISCA Interspeech 2016, San Francisco, United States, Sep. 2016."
                    },
                    {
                        "ref-fulltext": "S. Kayte, M. Mundada, and J. Gujrathi, \"Hidden markov model based speech synthesis: A review, \" Int. J. Comput. Appl., vol. 130, 2015.",
                        "@id": "4",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2015"},
                            "ref-title": {"ref-titletext": "Hidden markov model based speech synthesis: A review"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85056778273",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"voliss": {"@volume": "130"}},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Kayte",
                                    "ce:indexed-name": "Kayte S."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Mundada",
                                    "ce:indexed-name": "Mundada M."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Gujrathi",
                                    "ce:indexed-name": "Gujrathi J."
                                }
                            ]},
                            "ref-sourcetitle": "Int. J. Comput. Appl."
                        },
                        "ce:source-text": "S. Kayte, M. Mundada, and J. Gujrathi, \"Hidden markov model based speech synthesis: A review, \" Int. J. Comput. Appl., vol. 130, 2015."
                    },
                    {
                        "ref-fulltext": "F. Richardson, D. Reynolds, and N. Dehak, \"Deep neural network approaches to speaker and language recognition, \" IEEE Signal Process. Lett., vol. 22, no. 10, pp. 1671-1675, Oct. 2015.",
                        "@id": "5",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2015"},
                            "ref-title": {"ref-titletext": "Deep neural network approaches to speaker and language recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84928742875",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "22",
                                    "@issue": "10"
                                },
                                "pagerange": {
                                    "@first": "1671",
                                    "@last": "1675"
                                }
                            },
                            "ref-text": "Oct.",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "F.",
                                    "@_fa": "true",
                                    "ce:surname": "Richardson",
                                    "ce:indexed-name": "Richardson F."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "D.",
                                    "@_fa": "true",
                                    "ce:surname": "Reynolds",
                                    "ce:indexed-name": "Reynolds D."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "N.",
                                    "@_fa": "true",
                                    "ce:surname": "Dehak",
                                    "ce:indexed-name": "Dehak N."
                                }
                            ]},
                            "ref-sourcetitle": "IEEE Signal Process. Lett."
                        },
                        "ce:source-text": "F. Richardson, D. Reynolds, and N. Dehak, \"Deep neural network approaches to speaker and language recognition, \" IEEE Signal Process. Lett., vol. 22, no. 10, pp. 1671-1675, Oct. 2015."
                    },
                    {
                        "ref-fulltext": "L. Tian, J. Moore, and C. Lai, \"Emotion recognition in spontaneous and acted dialogues, \" presented at 2015 Int. Conf. Affect. Comput. and Intell. Interact. (ACII), Xi'an, Sep. 2015.",
                        "@id": "6",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2015"},
                            "ref-title": {"ref-titletext": "Emotion recognition in spontaneous and acted dialogues"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84964039845",
                                "@idtype": "SGR"
                            }},
                            "ref-text": "Xi'an, Sep.",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "L.",
                                    "@_fa": "true",
                                    "ce:surname": "Tian",
                                    "ce:indexed-name": "Tian L."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Moore",
                                    "ce:indexed-name": "Moore J."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "C.",
                                    "@_fa": "true",
                                    "ce:surname": "Lai",
                                    "ce:indexed-name": "Lai C."
                                }
                            ]},
                            "ref-sourcetitle": "2015 Int. Conf. Affect. Comput. And Intell. Interact. (ACII)"
                        },
                        "ce:source-text": "L. Tian, J. Moore, and C. Lai, \"Emotion recognition in spontaneous and acted dialogues, \" presented at 2015 Int. Conf. Affect. Comput. and Intell. Interact. (ACII), Xi'an, Sep. 2015."
                    },
                    {
                        "ref-fulltext": "F. Tao, and G. Liu, \"Advanced LSTM: A study about better time dependency modeling in emotion recognition, \" presented at ICASSP 2018-2018 IEEE Int. Conf. Acoust. Speech and Signal Processing (ICASSP), Apr. 2018, doi: 10. 1109/ICASSP. 2018. 8461750.",
                        "@id": "7",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2018"},
                            "ref-title": {"ref-titletext": "Advanced lstm: A study about better time dependency modeling in emotion recognition"},
                            "refd-itemidlist": {"itemid": [
                                {
                                    "$": "10.1109/ICASSP.2018.8461750",
                                    "@idtype": "DOI"
                                },
                                {
                                    "$": "85054240863",
                                    "@idtype": "SGR"
                                }
                            ]},
                            "ref-text": "Apr.",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "F.",
                                    "@_fa": "true",
                                    "ce:surname": "Tao",
                                    "ce:indexed-name": "Tao F."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "G.",
                                    "@_fa": "true",
                                    "ce:surname": "Liu",
                                    "ce:indexed-name": "Liu G."
                                }
                            ]},
                            "ref-sourcetitle": "ICASSP 2018-2018 IEEE Int. Conf. Acoust. Speech and Signal Processing (ICASSP)"
                        },
                        "ce:source-text": "F. Tao, and G. Liu, \"Advanced LSTM: A study about better time dependency modeling in emotion recognition, \" presented at ICASSP 2018-2018 IEEE Int. Conf. Acoust. Speech and Signal Processing (ICASSP), Apr. 2018, doi: 10. 1109/ICASSP. 2018. 8461750."
                    },
                    {
                        "ref-fulltext": "H. Fayek, M. Lech, and L. Cavedon, \"Evaluating deep learning architectures for speech emotion recognition, \" Neural Networks, vol. 92, pp. 60-68, Aug. 2017.",
                        "@id": "8",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2017"},
                            "ref-title": {"ref-titletext": "Evaluating deep learning architectures for speech emotion recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85017190163",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "92"},
                                "pagerange": {
                                    "@first": "60",
                                    "@last": "68"
                                }
                            },
                            "ref-text": "Aug.",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "H.",
                                    "@_fa": "true",
                                    "ce:surname": "Fayek",
                                    "ce:indexed-name": "Fayek H."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Lech",
                                    "ce:indexed-name": "Lech M."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "L.",
                                    "@_fa": "true",
                                    "ce:surname": "Cavedon",
                                    "ce:indexed-name": "Cavedon L."
                                }
                            ]},
                            "ref-sourcetitle": "Neural Networks"
                        },
                        "ce:source-text": "H. Fayek, M. Lech, and L. Cavedon, \"Evaluating deep learning architectures for speech emotion recognition, \" Neural Networks, vol. 92, pp. 60-68, Aug. 2017."
                    },
                    {
                        "ref-fulltext": "R. Chakraborty, M. Pandharipande, and S. Kopparapu, \"Spontaneous speech emotion recognition using prior knowledge, \" presented at 2016 23rd Int. Conf. Pattern Recognit. (ICPR), Cancún, México, Dec. 4-8, 2016.",
                        "@id": "9",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2016"},
                            "ref-title": {"ref-titletext": "Spontaneous speech emotion recognition using prior knowledge"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85019159791",
                                "@idtype": "SGR"
                            }},
                            "ref-text": "Cancún, México, Dec. 4-8",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "R.",
                                    "@_fa": "true",
                                    "ce:surname": "Chakraborty",
                                    "ce:indexed-name": "Chakraborty R."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Pandharipande",
                                    "ce:indexed-name": "Pandharipande M."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Kopparapu",
                                    "ce:indexed-name": "Kopparapu S."
                                }
                            ]},
                            "ref-sourcetitle": "2016 23rd Int. Conf. Pattern Recognit. (ICPR)"
                        },
                        "ce:source-text": "R. Chakraborty, M. Pandharipande, and S. Kopparapu, \"Spontaneous speech emotion recognition using prior knowledge, \" presented at 2016 23rd Int. Conf. Pattern Recognit. (ICPR), Cancún, México, Dec. 4-8, 2016."
                    },
                    {
                        "ref-fulltext": "C. Wu, J. Lin, and W. Wei, \"Survey on audiovisual emotion recognition: databases, features, and data fusion strategies, \" APSIPA Trans. Signal Inf. Process, vol. 3, 2014, doi:10. 1017/ATSIP. 2014. 11.",
                        "@id": "10",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2014"},
                            "ref-title": {"ref-titletext": "Survey on audiovisual emotion recognition: Databases, features, and data fusion strategies"},
                            "refd-itemidlist": {"itemid": [
                                {
                                    "$": "10.1017/ATSIP.2014.11",
                                    "@idtype": "DOI"
                                },
                                {
                                    "$": "84956776418",
                                    "@idtype": "SGR"
                                }
                            ]},
                            "ref-volisspag": {"voliss": {"@volume": "3"}},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "C.",
                                    "@_fa": "true",
                                    "ce:surname": "Wu",
                                    "ce:indexed-name": "Wu C."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Lin",
                                    "ce:indexed-name": "Lin J."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "W.",
                                    "@_fa": "true",
                                    "ce:surname": "Wei",
                                    "ce:indexed-name": "Wei W."
                                }
                            ]},
                            "ref-sourcetitle": "APSIPA Trans. Signal Inf. Process"
                        },
                        "ce:source-text": "C. Wu, J. Lin, and W. Wei, \"Survey on audiovisual emotion recognition: databases, features, and data fusion strategies, \" APSIPA Trans. Signal Inf. Process, vol. 3, 2014, doi:10. 1017/ATSIP. 2014. 11."
                    },
                    {
                        "ref-fulltext": "F. Ringeval, B. Schuller, M. Valstar, S. Jaiswal, E. Marchi, et al. \"Av+ ec 2015: the first affect recognition challenge bridging across audio, video, and physiological data, \" in Proc. 5th Int. Workshop Audio/Vis. Emotion Challenge, Brisbane, Australia, 2015.",
                        "@id": "11",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2015"},
                            "ref-title": {"ref-titletext": "Av+ ec 2015: The first affect recognition challenge bridging across audio, video, and physiological data"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84960854232",
                                "@idtype": "SGR"
                            }},
                            "ref-text": "Brisbane, Australia",
                            "ref-authors": {
                                "author": [
                                    {
                                        "@seq": "1",
                                        "ce:initials": "F.",
                                        "@_fa": "true",
                                        "ce:surname": "Ringeval",
                                        "ce:indexed-name": "Ringeval F."
                                    },
                                    {
                                        "@seq": "2",
                                        "ce:initials": "B.",
                                        "@_fa": "true",
                                        "ce:surname": "Schuller",
                                        "ce:indexed-name": "Schuller B."
                                    },
                                    {
                                        "@seq": "3",
                                        "ce:initials": "M.",
                                        "@_fa": "true",
                                        "ce:surname": "Valstar",
                                        "ce:indexed-name": "Valstar M."
                                    },
                                    {
                                        "@seq": "4",
                                        "ce:initials": "S.",
                                        "@_fa": "true",
                                        "ce:surname": "Jaiswal",
                                        "ce:indexed-name": "Jaiswal S."
                                    },
                                    {
                                        "@seq": "5",
                                        "ce:initials": "E.",
                                        "@_fa": "true",
                                        "ce:surname": "Marchi",
                                        "ce:indexed-name": "Marchi E."
                                    }
                                ],
                                "et-al": null
                            },
                            "ref-sourcetitle": "Proc. 5th Int. Workshop Audio/Vis. Emotion Challenge"
                        },
                        "ce:source-text": "F. Ringeval, B. Schuller, M. Valstar, S. Jaiswal, E. Marchi, et al. \"Av+ ec 2015: the first affect recognition challenge bridging across audio, video, and physiological data, \" in Proc. 5th Int. Workshop Audio/Vis. Emotion Challenge, Brisbane, Australia, 2015."
                    },
                    {
                        "ref-fulltext": "H. Cao, R. Verma, and A. Nenkova, \"Speaker-sensitive emotion recognition via ranking: studies on acted and spontaneous speech, \" Comput. Speech Lang., vol. 28, no. 1, pp. 186-202.",
                        "@id": "12",
                        "ref-info": {
                            "ref-title": {"ref-titletext": "Speaker-sensitive emotion recognition via ranking: Studies on acted and spontaneous speech"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84908548915",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "28",
                                    "@issue": "1"
                                },
                                "pagerange": {
                                    "@first": "186",
                                    "@last": "202"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "H.",
                                    "@_fa": "true",
                                    "ce:surname": "Cao",
                                    "ce:indexed-name": "Cao H."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "R.",
                                    "@_fa": "true",
                                    "ce:surname": "Verma",
                                    "ce:indexed-name": "Verma R."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Nenkova",
                                    "ce:indexed-name": "Nenkova A."
                                }
                            ]},
                            "ref-sourcetitle": "Comput. Speech Lang."
                        },
                        "ce:source-text": "H. Cao, R. Verma, and A. Nenkova, \"Speaker-sensitive emotion recognition via ranking: studies on acted and spontaneous speech, \" Comput. Speech Lang., vol. 28, no. 1, pp. 186-202."
                    },
                    {
                        "ref-fulltext": "K. Truong, D. van Leeuwen, and F. de Jong, \"Speech-based recognition of self-reported and observed emotion in a dimensional space, \" Speech Commun., vol. 54, no. 9, pp. 1049-1063, Nov. 2012.",
                        "@id": "13",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2012"},
                            "ref-title": {"ref-titletext": "Speech-based recognition of self-reported and observed emotion in a dimensional space"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84862851965",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "54",
                                    "@issue": "9"
                                },
                                "pagerange": {
                                    "@first": "1049",
                                    "@last": "1063"
                                }
                            },
                            "ref-text": "Nov.",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "K.",
                                    "@_fa": "true",
                                    "ce:surname": "Truong",
                                    "ce:indexed-name": "Truong K."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "D.",
                                    "@_fa": "true",
                                    "ce:surname": "Van Leeuwen",
                                    "ce:indexed-name": "Van Leeuwen D."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "F.",
                                    "@_fa": "true",
                                    "ce:surname": "De Jong",
                                    "ce:indexed-name": "De Jong F."
                                }
                            ]},
                            "ref-sourcetitle": "Speech Commun."
                        },
                        "ce:source-text": "K. Truong, D. van Leeuwen, and F. de Jong, \"Speech-based recognition of self-reported and observed emotion in a dimensional space, \" Speech Commun., vol. 54, no. 9, pp. 1049-1063, Nov. 2012."
                    },
                    {
                        "ref-fulltext": "F. Burkhardt, A. Paeschke, M. Rolfes, W. Sendlmeier, and B. Weiss, \"A database of german emotional speech, \" presented at INTERSPEECH 2005-Eurospeech, 9th Eur. Conf. Speech Comm. Technol., Lisbon, Portugal, Sep. 4-8, 2005.",
                        "@id": "14",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2005"},
                            "ref-title": {"ref-titletext": "A database of German emotional speech"},
                            "refd-itemidlist": {"itemid": {
                                "$": "33745202280",
                                "@idtype": "SGR"
                            }},
                            "ref-text": "Lisbon, Portugal, Sep. 4-8",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "F.",
                                    "@_fa": "true",
                                    "ce:surname": "Burkhardt",
                                    "ce:indexed-name": "Burkhardt F."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Paeschke",
                                    "ce:indexed-name": "Paeschke A."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Rolfes",
                                    "ce:indexed-name": "Rolfes M."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "W.",
                                    "@_fa": "true",
                                    "ce:surname": "Sendlmeier",
                                    "ce:indexed-name": "Sendlmeier W."
                                },
                                {
                                    "@seq": "5",
                                    "ce:initials": "B.",
                                    "@_fa": "true",
                                    "ce:surname": "Weiss",
                                    "ce:indexed-name": "Weiss B."
                                }
                            ]},
                            "ref-sourcetitle": "INTERSPEECH 2005-Eurospeech, 9th Eur. Conf. Speech Comm. Technol."
                        },
                        "ce:source-text": "F. Burkhardt, A. Paeschke, M. Rolfes, W. Sendlmeier, and B. Weiss, \"A database of german emotional speech, \" presented at INTERSPEECH 2005-Eurospeech, 9th Eur. Conf. Speech Comm. Technol., Lisbon, Portugal, Sep. 4-8, 2005."
                    },
                    {
                        "ref-fulltext": "S. Haq, P. Jackson, and J. Edge, \"Audio-visual feature selection and reduction for emotion classification, \" in Proc. Int. Conf. on Auditory-Vis. Speech Process., Sep. 2008.",
                        "@id": "15",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2008"},
                            "ref-title": {"ref-titletext": "Audio-visual feature selection and reduction for emotion classification"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85133493946",
                                "@idtype": "SGR"
                            }},
                            "ref-text": "Sep.",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Haq",
                                    "ce:indexed-name": "Haq S."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "P.",
                                    "@_fa": "true",
                                    "ce:surname": "Jackson",
                                    "ce:indexed-name": "Jackson P."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Edge",
                                    "ce:indexed-name": "Edge J."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. Int. Conf. On Auditory-Vis. Speech Process."
                        },
                        "ce:source-text": "S. Haq, P. Jackson, and J. Edge, \"Audio-visual feature selection and reduction for emotion classification, \" in Proc. Int. Conf. on Auditory-Vis. Speech Process., Sep. 2008."
                    },
                    {
                        "ref-fulltext": "J. Hansen, \"SUSAS LDC99S78. \" 1999. Distributed by Philadelphia: Linguistic Data Consortium. https://catalog. ldc. upenn. edu/LDC99S78.",
                        "@id": "16",
                        "ref-info": {
                            "ref-website": {"ce:e-address": {
                                "$": "https://catalog.ldc.upenn.edu/LDC99S78",
                                "@type": "email"
                            }},
                            "ref-title": {"ref-titletext": "SUSAS ldc99s78."},
                            "refd-itemidlist": {"itemid": {
                                "$": "85091853217",
                                "@idtype": "SGR"
                            }},
                            "ref-authors": {"author": [{
                                "@seq": "1",
                                "ce:initials": "J.",
                                "@_fa": "true",
                                "ce:surname": "Hansen",
                                "ce:indexed-name": "Hansen J."
                            }]},
                            "ref-sourcetitle": "1999. Distributed by Philadelphia: Linguistic Data Consortium"
                        },
                        "ce:source-text": "J. Hansen, \"SUSAS LDC99S78. \" 1999. Distributed by Philadelphia: Linguistic Data Consortium. https://catalog. ldc. upenn. edu/LDC99S78."
                    },
                    {
                        "ref-fulltext": "K. Wang, \"Time-frequency feature representation using multi-resolution texture analysis and acoustic activity detector for real-life speech emotion recognition, \" Sensors (Basel), vol. 15, no. 1, pp. 1458-1478, Jan. 2015.",
                        "@id": "17",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2015"},
                            "ref-title": {"ref-titletext": "Time-frequency feature representation using multi-resolution texture analysis and acoustic activity detector for real-life speech emotion recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84921303869",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "15",
                                    "@issue": "1"
                                },
                                "pagerange": {
                                    "@first": "1458",
                                    "@last": "1478"
                                }
                            },
                            "ref-text": "Jan.",
                            "ref-authors": {"author": [{
                                "@seq": "1",
                                "ce:initials": "K.",
                                "@_fa": "true",
                                "ce:surname": "Wang",
                                "ce:indexed-name": "Wang K."
                            }]},
                            "ref-sourcetitle": "Sensors (Basel)"
                        },
                        "ce:source-text": "K. Wang, \"Time-frequency feature representation using multi-resolution texture analysis and acoustic activity detector for real-life speech emotion recognition, \" Sensors (Basel), vol. 15, no. 1, pp. 1458-1478, Jan. 2015."
                    },
                    {
                        "ref-fulltext": "L. Vidrascu, and L. Devillers, \"Real-life emotion representation and detection in call centers data, \" in Proc. 1st Int. Conf., ACII 2005, in Lecture Notes in Computer Science, vol. 3784, Oct. 2005, pp. 739-746.",
                        "@id": "18",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2005"},
                            "ref-title": {"ref-titletext": "Real-life emotion representation and detection in call centers data"},
                            "refd-itemidlist": {"itemid": {
                                "$": "33646763892",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "3784"},
                                "pagerange": {
                                    "@first": "739",
                                    "@last": "746"
                                }
                            },
                            "ref-text": "Oct.",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "L.",
                                    "@_fa": "true",
                                    "ce:surname": "Vidrascu",
                                    "ce:indexed-name": "Vidrascu L."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "L.",
                                    "@_fa": "true",
                                    "ce:surname": "Devillers",
                                    "ce:indexed-name": "Devillers L."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. 1st Int. Conf., ACII 2005 in Lecture Notes in Computer Science"
                        },
                        "ce:source-text": "L. Vidrascu, and L. Devillers, \"Real-life emotion representation and detection in call centers data, \" in Proc. 1st Int. Conf., ACII 2005, in Lecture Notes in Computer Science, vol. 3784, Oct. 2005, pp. 739-746."
                    },
                    {
                        "ref-fulltext": "M. Ayadi, M. Kamel, and F. Karray, \"Survey on speech emotion recognition: features, classification schemes, and databases, \" Pattern Recognition, vol. 44, no. 3, 2011, pp. 572-587.",
                        "@id": "19",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2011"},
                            "ref-title": {"ref-titletext": "Survey on speech emotion recognition: Features, classification schemes, and databases"},
                            "refd-itemidlist": {"itemid": {
                                "$": "78649328053",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "44",
                                    "@issue": "3"
                                },
                                "pagerange": {
                                    "@first": "572",
                                    "@last": "587"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Ayadi",
                                    "ce:indexed-name": "Ayadi M."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Kamel",
                                    "ce:indexed-name": "Kamel M."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "F.",
                                    "@_fa": "true",
                                    "ce:surname": "Karray",
                                    "ce:indexed-name": "Karray F."
                                }
                            ]},
                            "ref-sourcetitle": "Pattern Recognition"
                        },
                        "ce:source-text": "M. Ayadi, M. Kamel, and F. Karray, \"Survey on speech emotion recognition: features, classification schemes, and databases, \" Pattern Recognition, vol. 44, no. 3, 2011, pp. 572-587."
                    },
                    {
                        "ref-fulltext": "J. Pittam, and K. Scherer, \"Vocal expression and communication of emotion, \" in Handbook of Emotions, New York, NY, US: Guilford Press, 1993, pp. 185-197.",
                        "@id": "20",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "1993"},
                            "ref-title": {"ref-titletext": "Vocal expression and communication of emotion"},
                            "refd-itemidlist": {"itemid": {
                                "$": "0002351519",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "185",
                                "@last": "197"
                            }},
                            "ref-text": "New York, NY, US: Guilford Press",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Pittam",
                                    "ce:indexed-name": "Pittam J."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "K.",
                                    "@_fa": "true",
                                    "ce:surname": "Scherer",
                                    "ce:indexed-name": "Scherer K."
                                }
                            ]},
                            "ref-sourcetitle": "Handbook of Emotions"
                        },
                        "ce:source-text": "J. Pittam, and K. Scherer, \"Vocal expression and communication of emotion, \" in Handbook of Emotions, New York, NY, US: Guilford Press, 1993, pp. 185-197."
                    },
                    {
                        "ref-fulltext": "P. Tischer, \" Self-consciousness and emotional expression, \" Dissertations from ProQuest, 1994. [Online]. Available: https://scholarlyrepository. miami. edu/dissertations/3243",
                        "@id": "21",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "1994"},
                            "ref-website": {"ce:e-address": {
                                "$": "https://scholarlyrepository.miami.edu/dissertations/3243",
                                "@type": "email"
                            }},
                            "ref-title": {"ref-titletext": "Self-consciousness and emotional expression"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85074228262",
                                "@idtype": "SGR"
                            }},
                            "ref-authors": {"author": [{
                                "@seq": "1",
                                "ce:initials": "P.",
                                "@_fa": "true",
                                "ce:surname": "Tischer",
                                "ce:indexed-name": "Tischer P."
                            }]},
                            "ref-sourcetitle": "Dissertations from ProQuest"
                        },
                        "ce:source-text": "P. Tischer, \" Self-consciousness and emotional expression, \" Dissertations from ProQuest, 1994. [Online]. Available: https://scholarlyrepository. miami. edu/dissertations/3243"
                    },
                    {
                        "ref-fulltext": "E. Ramdinmawii, A. Mohanta, and V. K. Mittal, \"Emotion recognition from speech signal, \" in TENCON 2017-2017 IEEE Region 10 Conference, Penang, 2017, pp. 1562-1567, doi: 10. 1109/TENCON. 2017. 8228105.",
                        "@id": "22",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2017"},
                            "ref-title": {"ref-titletext": "Emotion recognition from speech signal"},
                            "refd-itemidlist": {"itemid": [
                                {
                                    "$": "10.1109/TENCON.2017.8228105",
                                    "@idtype": "DOI"
                                },
                                {
                                    "$": "85044190428",
                                    "@idtype": "SGR"
                                }
                            ]},
                            "ref-volisspag": {"pagerange": {
                                "@first": "1562",
                                "@last": "1567"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "E.",
                                    "@_fa": "true",
                                    "ce:surname": "Ramdinmawii",
                                    "ce:indexed-name": "Ramdinmawii E."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Mohanta",
                                    "ce:indexed-name": "Mohanta A."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "V.K.",
                                    "@_fa": "true",
                                    "ce:surname": "Mittal",
                                    "ce:indexed-name": "Mittal V.K."
                                }
                            ]},
                            "ref-sourcetitle": "TENCON 2017-2017 IEEE Region 10 Conference, Penang"
                        },
                        "ce:source-text": "E. Ramdinmawii, A. Mohanta, and V. K. Mittal, \"Emotion recognition from speech signal, \" in TENCON 2017-2017 IEEE Region 10 Conference, Penang, 2017, pp. 1562-1567, doi: 10. 1109/TENCON. 2017. 8228105."
                    },
                    {
                        "ref-fulltext": "B. Schuller, S. Steidl, and A. Batliner, \"INTERSPEECH 2009 Emotion challenge feature set (IS): low-level descriptors (LLD) and functionals, \" in INTERSPEECH, 2009.",
                        "@id": "23",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2009"},
                            "ref-title": {"ref-titletext": "INTERSPEECH 2009 emotion challenge feature set (is): Low-level descriptors (lld) and functionals"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84920277540",
                                "@idtype": "SGR"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "B.",
                                    "@_fa": "true",
                                    "ce:surname": "Schuller",
                                    "ce:indexed-name": "Schuller B."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Steidl",
                                    "ce:indexed-name": "Steidl S."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Batliner",
                                    "ce:indexed-name": "Batliner A."
                                }
                            ]},
                            "ref-sourcetitle": "INTERSPEECH"
                        },
                        "ce:source-text": "B. Schuller, S. Steidl, and A. Batliner, \"INTERSPEECH 2009 Emotion challenge feature set (IS): low-level descriptors (LLD) and functionals, \" in INTERSPEECH, 2009."
                    },
                    {
                        "ref-fulltext": "W. Saewong, and J. Mongkolnavin, \"Classification of anger voice in call center dialog, \" in 2019 16th International Joint Conference on Computer Science and Software Engineering (JCSSE), Chonburi, Thailand, 2019, pp. 298-302, doi: 10. 1109/JCSSE. 2019. 8864217.",
                        "@id": "24",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2019"},
                            "ref-title": {"ref-titletext": "Classification of anger voice in call center dialog"},
                            "refd-itemidlist": {"itemid": [
                                {
                                    "$": "10.1109/JCSSE.2019.8864217",
                                    "@idtype": "DOI"
                                },
                                {
                                    "$": "85074226436",
                                    "@idtype": "SGR"
                                }
                            ]},
                            "ref-volisspag": {"pagerange": {
                                "@first": "298",
                                "@last": "302"
                            }},
                            "ref-text": "Chonburi, Thailand",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "W.",
                                    "@_fa": "true",
                                    "ce:surname": "Saewong",
                                    "ce:indexed-name": "Saewong W."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Mongkolnavin",
                                    "ce:indexed-name": "Mongkolnavin J."
                                }
                            ]},
                            "ref-sourcetitle": "2019 16th International Joint Conference on Computer Science and Software Engineering (JCSSE)"
                        },
                        "ce:source-text": "W. Saewong, and J. Mongkolnavin, \"Classification of anger voice in call center dialog, \" in 2019 16th International Joint Conference on Computer Science and Software Engineering (JCSSE), Chonburi, Thailand, 2019, pp. 298-302, doi: 10. 1109/JCSSE. 2019. 8864217."
                    },
                    {
                        "ref-fulltext": "B. Schuller, S. Steidl, A. Batliner, F. Burkhardt, L. Devillers, C. Müller, and S. Narayanan, \" INTERSPEECH 2010 Paralinguistic challenge, \" in INTERSPEECH, 2010.",
                        "@id": "25",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2010"},
                            "ref-title": {"ref-titletext": "INTERSPEECH 2010 paralinguistic challenge"},
                            "refd-itemidlist": {"itemid": {
                                "$": "79954999224",
                                "@idtype": "SGR"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "B.",
                                    "@_fa": "true",
                                    "ce:surname": "Schuller",
                                    "ce:indexed-name": "Schuller B."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Steidl",
                                    "ce:indexed-name": "Steidl S."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Batliner",
                                    "ce:indexed-name": "Batliner A."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "F.",
                                    "@_fa": "true",
                                    "ce:surname": "Burkhardt",
                                    "ce:indexed-name": "Burkhardt F."
                                },
                                {
                                    "@seq": "5",
                                    "ce:initials": "L.",
                                    "@_fa": "true",
                                    "ce:surname": "Devillers",
                                    "ce:indexed-name": "Devillers L."
                                },
                                {
                                    "@seq": "6",
                                    "ce:initials": "C.",
                                    "@_fa": "true",
                                    "ce:surname": "Müller",
                                    "ce:indexed-name": "Muller C."
                                },
                                {
                                    "@seq": "7",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Narayanan",
                                    "ce:indexed-name": "Narayanan S."
                                }
                            ]},
                            "ref-sourcetitle": "INTERSPEECH"
                        },
                        "ce:source-text": "B. Schuller, S. Steidl, A. Batliner, F. Burkhardt, L. Devillers, C. Müller, and S. Narayanan, \" INTERSPEECH 2010 Paralinguistic challenge, \" in INTERSPEECH, 2010."
                    },
                    {
                        "ref-fulltext": "F. Eyben, F. Weninger, F. Gross, and B. Schuller, \"Recent developments in openSMILE, the munich open-source multimedia feature extractor, \" in MM 2013-Proc. 2013 ACM Multimedia Conf., Oct. 2013, pp. 835-838.",
                        "@id": "26",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2013"},
                            "ref-title": {"ref-titletext": "Recent developments in opensmile, the munich open-source multimedia feature extractor"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84887494391",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "835",
                                "@last": "838"
                            }},
                            "ref-text": "Oct.",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "F.",
                                    "@_fa": "true",
                                    "ce:surname": "Eyben",
                                    "ce:indexed-name": "Eyben F."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "F.",
                                    "@_fa": "true",
                                    "ce:surname": "Weninger",
                                    "ce:indexed-name": "Weninger F."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "F.",
                                    "@_fa": "true",
                                    "ce:surname": "Gross",
                                    "ce:indexed-name": "Gross F."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "B.",
                                    "@_fa": "true",
                                    "ce:surname": "Schuller",
                                    "ce:indexed-name": "Schuller B."
                                }
                            ]},
                            "ref-sourcetitle": "MM 2013-Proc. 2013 ACM Multimedia Conf."
                        },
                        "ce:source-text": "F. Eyben, F. Weninger, F. Gross, and B. Schuller, \"Recent developments in openSMILE, the munich open-source multimedia feature extractor, \" in MM 2013-Proc. 2013 ACM Multimedia Conf., Oct. 2013, pp. 835-838."
                    }
                ]
            }}
        }
    },
    "affiliation": {
        "affiliation-city": "Bangkok",
        "@id": "60199583",
        "affilname": "Chulalongkorn Business School",
        "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60199583",
        "affiliation-country": "Thailand"
    },
    "coredata": {
        "srctype": "p",
        "eid": "2-s2.0-85091889483",
        "dc:description": "Call center is a department that is most relevant to audio data usage. One of its major tasks is to monitor customers' anguish because it has a negative impact on the organization. One challenging task is to develop a model that can predict whether a customer is getting angry in the next turn of conversation. Such model can assist agents in taking appropriate action(s) to prevent the incidents. In this study, we investigate an approach to build an anger prediction model from customers' voice in call center dialogs. To create the model requires 5 processes: (1) Customer's turn extraction (2) Emotion annotation (3) Voice feature selection (4) Data pre-processing for long short-term memory networks, and (5) Anger prediction modeling. Five long short-term memory networks were built with the time series data sets of 1, 2, 3, 4, and 5 consecutive turns. The experimental results showed that the long short-term memory network built with the 3-consecutive turn data has promising performance in aspect of Average Precision and False Negative Rate when compared to the random and good guess benchmarks.",
        "prism:coverDate": "2020-06-01",
        "prism:aggregationType": "Conference Proceeding",
        "prism:url": "https://api.elsevier.com/content/abstract/scopus_id/85091889483",
        "dc:creator": {"author": [{
            "ce:given-name": "Janjao",
            "preferred-name": {
                "ce:given-name": "Janjao",
                "ce:initials": "J.",
                "ce:surname": "Mongkolnavin",
                "ce:indexed-name": "Mongkolnavin J."
            },
            "@seq": "1",
            "ce:initials": "J.",
            "@_fa": "true",
            "affiliation": {
                "@id": "60199583",
                "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60199583"
            },
            "ce:surname": "Mongkolnavin",
            "@auid": "25922814600",
            "author-url": "https://api.elsevier.com/content/author/author_id/25922814600",
            "ce:indexed-name": "Mongkolnavin J."
        }]},
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/abstract/scopus_id/85091889483"
            },
            {
                "@_fa": "true",
                "@rel": "scopus",
                "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85091889483&origin=inward"
            },
            {
                "@_fa": "true",
                "@rel": "scopus-citedby",
                "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85091889483&origin=inward"
            }
        ],
        "prism:isbn": "9781728164861",
        "source-id": "21101022467",
        "citedby-count": "1",
        "subtype": "cp",
        "dc:title": "Prediction of Forthcoming Anger of Customer in Call Center Dialogs",
        "openaccess": "0",
        "publishercopyright": "© 2020 IEEE.",
        "article-number": "9158120",
        "subtypeDescription": "Conference Paper",
        "prism:publicationName": "17th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology, ECTI-CON 2020",
        "prism:pageRange": "779-783",
        "prism:endingPage": "783",
        "openaccessFlag": "false",
        "prism:doi": "10.1109/ECTI-CON49241.2020.9158120",
        "prism:startingPage": "779",
        "dc:identifier": "SCOPUS_ID:85091889483",
        "dc:publisher": "Institute of Electrical and Electronics Engineers Inc."
    },
    "idxterms": {"mainterm": [
        {
            "$": "Audio data",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Call centers",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Data preprocessing",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "False negative rate",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Prediction model",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Short term memory",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Time-series data",
            "@weight": "b",
            "@candidate": "n"
        }
    ]},
    "language": {"@xml:lang": "eng"},
    "authkeywords": {"author-keyword": [
        {
            "@_fa": "true",
            "$": "Anger Prediction"
        },
        {
            "@_fa": "true",
            "$": "Call center dialogs"
        },
        {
            "@_fa": "true",
            "$": "Emotion Recognition"
        },
        {
            "@_fa": "true",
            "$": "Long Short-term Memory Networks"
        },
        {
            "@_fa": "true",
            "$": "Machine Learning"
        },
        {
            "@_fa": "true",
            "$": "Recurrent Neural Networks"
        }
    ]},
    "subject-areas": {"subject-area": [
        {
            "@_fa": "true",
            "$": "Artificial Intelligence",
            "@code": "1702",
            "@abbrev": "COMP"
        },
        {
            "@_fa": "true",
            "$": "Computer Networks and Communications",
            "@code": "1705",
            "@abbrev": "COMP"
        },
        {
            "@_fa": "true",
            "$": "Hardware and Architecture",
            "@code": "1708",
            "@abbrev": "COMP"
        },
        {
            "@_fa": "true",
            "$": "Software",
            "@code": "1712",
            "@abbrev": "COMP"
        },
        {
            "@_fa": "true",
            "$": "Information Systems and Management",
            "@code": "1802",
            "@abbrev": "DECI"
        },
        {
            "@_fa": "true",
            "$": "Electrical and Electronic Engineering",
            "@code": "2208",
            "@abbrev": "ENGI"
        },
        {
            "@_fa": "true",
            "$": "Instrumentation",
            "@code": "3105",
            "@abbrev": "PHYS"
        }
    ]},
    "authors": {"author": [
        {
            "ce:given-name": "Janjao",
            "preferred-name": {
                "ce:given-name": "Janjao",
                "ce:initials": "J.",
                "ce:surname": "Mongkolnavin",
                "ce:indexed-name": "Mongkolnavin J."
            },
            "@seq": "1",
            "ce:initials": "J.",
            "@_fa": "true",
            "affiliation": {
                "@id": "60199583",
                "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60199583"
            },
            "ce:surname": "Mongkolnavin",
            "@auid": "25922814600",
            "author-url": "https://api.elsevier.com/content/author/author_id/25922814600",
            "ce:indexed-name": "Mongkolnavin J."
        },
        {
            "ce:given-name": "Widakorn",
            "preferred-name": {
                "ce:given-name": "Widakorn",
                "ce:initials": "W.",
                "ce:surname": "Saewong",
                "ce:indexed-name": "Saewong W."
            },
            "@seq": "2",
            "ce:initials": "W.",
            "@_fa": "true",
            "affiliation": {
                "@id": "60199583",
                "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60199583"
            },
            "ce:surname": "Saewong",
            "@auid": "57211507692",
            "author-url": "https://api.elsevier.com/content/author/author_id/57211507692",
            "ce:indexed-name": "Saewong W."
        }
    ]}
}}